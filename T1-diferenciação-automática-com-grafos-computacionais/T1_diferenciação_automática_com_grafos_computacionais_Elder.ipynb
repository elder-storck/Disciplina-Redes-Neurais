{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elder-storck/Disciplina-Redes-Neurais/blob/main/T1-diferencia%C3%A7%C3%A3o-autom%C3%A1tica-com-grafos-computacionais/T1_diferencia%C3%A7%C3%A3o_autom%C3%A1tica_com_grafos_computacionais_Elder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a622311",
      "metadata": {
        "id": "0a622311"
      },
      "source": [
        "# Nova se√ß√£o\n",
        "# Trabalho 1: Diferencia√ß√£o Autom√°tica com Grafos Computacionais\n",
        "\n",
        "## Informa√ß√µes Gerais\n",
        "\n",
        "- Data de Entrega: 28/11/2025\n",
        "- Pontua√ß√£o: 10 pontos\n",
        "- O trabalho deve ser feito individualmente.\n",
        "- A entrega do trabalho deve ser realizada via sistema testr.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81844651",
      "metadata": {
        "id": "81844651"
      },
      "source": [
        "## Especifica√ß√£o\n",
        "\n",
        "‚ö†Ô∏è *Esta explica√ß√£o assume que voc√™ leu e entendeu os slides sobre grafos computacionais.*\n",
        "\n",
        "O trabalho consiste em implementar um sistema de diferencia√ß√£o autom√°tica usando grafos computacionais e utilizar este sistema para resolver um conjunto de problemas.\n",
        "\n",
        "Para isto, devem ser definidos um tipo Tensor para representar dados (similares aos arrays do numpy) e opera√ß√µes (e.g., soma, subtra√ß√£o, etc.) que geram tensores como sa√≠da.\n",
        "\n",
        "Sempre que uma opera√ß√£o √© realizada, √© armazenado no tensor de sa√≠da refer√™ncias para os seus pais, isto √©, os valores usados como entrada para a opera√ß√£o.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faacc1a1",
      "metadata": {
        "id": "faacc1a1"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19261d1f",
      "metadata": {
        "id": "19261d1f"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Union, Any\n",
        "from collections.abc import Iterable\n",
        "from abc import ABC, abstractmethod\n",
        "import numbers\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style('whitegrid')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f284f531",
      "metadata": {
        "id": "f284f531"
      },
      "source": [
        "### Classe NameManager"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fd00b34",
      "metadata": {
        "id": "8fd00b34"
      },
      "source": [
        "A classe NameManager prov√™ uma forma conveniente de dar nomes intuitivos para tensores que resultam de opera√ß√µes. A id√©ia √© tornar mais f√°cil para o usu√°rio das demais classes qual opera√ß√£o gerou qual tensor. Ela prov√™ os seguintes m√©todos p√∫blicos:\n",
        "\n",
        "- reset(): reinicia o sistema de gest√£o de nomes.\n",
        "- new(<basename>: str): retorna um nome √∫nico a partir do nome de base passado como argumento.\n",
        "  \n",
        "Como indicado no exemplo abaixo da classe, a id√©ia geral √© que uma sequ√™ncia de opera√ß√µes √© feita, os nomes dos tensores sejam os nomes das opera√ß√µes seguidos de um n√∫mero. Se forem feitas 3 opera√ß√µes de soma e uma de multiplica√ß√£o, seus tensores de sa√≠da ter√£o os nomes \"add:0\", \"add:1\", \"add:2\" e \"prod:0\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "162272a0",
      "metadata": {
        "tags": [
          "name_manager"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "162272a0",
        "outputId": "29ec8f09-649f-49bc-fb8d-3153c451fcb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "add:0\n",
            "in:0\n",
            "add:1\n",
            "add:2\n",
            "in:1\n",
            "prod:0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class NameManager:\n",
        "    _counts = {}\n",
        "\n",
        "    @staticmethod\n",
        "    def reset():\n",
        "        NameManager._counts = {}\n",
        "\n",
        "    @staticmethod\n",
        "    def _count(name):\n",
        "        if name not in NameManager._counts:\n",
        "            NameManager._counts[name] = 0\n",
        "        count = NameManager._counts[name]\n",
        "        return count\n",
        "\n",
        "    @staticmethod\n",
        "    def _inc_count(name):\n",
        "        assert name in NameManager._counts, f'Name {name} is not registered.'\n",
        "        NameManager._counts[name] += 1\n",
        "\n",
        "    @staticmethod\n",
        "    def new(name: str):\n",
        "        count = NameManager._count(name)\n",
        "        tensor_name = f\"{name}:{count}\"\n",
        "        NameManager._inc_count(name)\n",
        "        return tensor_name\n",
        "\n",
        "# exemplo de uso\n",
        "print(NameManager.new('add'))\n",
        "print(NameManager.new('in'))\n",
        "print(NameManager.new('add'))\n",
        "print(NameManager.new('add'))\n",
        "print(NameManager.new('in'))\n",
        "print(NameManager.new('prod'))\n",
        "\n",
        "NameManager.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e69485a9",
      "metadata": {
        "id": "e69485a9"
      },
      "source": [
        "### Classe Tensor\n",
        "\n",
        "Deve ser criada uma classe `Tensor` representando um array multidimensional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "448496d7",
      "metadata": {
        "tags": [
          "tensor"
        ],
        "id": "448496d7"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Tensor:\n",
        "    \"\"\"Classe representando um array multidimensional.\n",
        "\n",
        "    Atributos:\n",
        "\n",
        "    - _arr  (privado): dados internos do tensor como\n",
        "        um array do numpy com 2 dimens√µes (ver Regras)\n",
        "\n",
        "    - _parents (privado): lista de tensores que foram\n",
        "        usados como argumento para a opera√ß√£o que gerou o\n",
        "        tensor. Ser√° vazia se o tensor foi inicializado com\n",
        "        valores diretamente. Por exemplo, se o tensor foi\n",
        "        resultado da opera√ß√£o a + b entre os tensores a e b,\n",
        "        _parents = [a, b].\n",
        "\n",
        "    - requires_grad (p√∫blico): indica se devem ser\n",
        "        calculados gradientes para o tensor ou n√£o.\n",
        "\n",
        "    - grad (p√∫blico): Tensor representando o gradiente.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 # Dados do tensor. Al√©m dos tipos listados,\n",
        "                 # arr tamb√©m pode ser do tipo Tensor.\n",
        "                 arr: Union[np.ndarray, list, numbers.Number, Any],\n",
        "                 # Entradas da operacao que gerou o tensor.\n",
        "                 # Deve ser uma lista de itens do tipo Tensor.\n",
        "                 parents: list[Any] = [],\n",
        "                 # se o tensor requer o calculo de gradientes ou nao\n",
        "                 requires_grad: bool = True,\n",
        "                 # nome do tensor\n",
        "                 name: str = '',\n",
        "                 # refer√™ncia para um objeto do tipo Operation (ou\n",
        "                 # subclasse) indicando qual opera√ß√£o gerou este\n",
        "                 # tensor. Este objeto tamb√©m possui um m√©todo\n",
        "                 # para calcular a derivada da opera√ß√£o.\n",
        "                 operation=None):\n",
        "        \"\"\"Construtor\n",
        "\n",
        "        O construtor deve permitir a criacao de tensores das seguintes formas:\n",
        "\n",
        "            # a partir de escalares\n",
        "            x = Tensor(3)\n",
        "\n",
        "            # a partir de listas\n",
        "            x = Tensor([1,2,3])\n",
        "\n",
        "            # a partir de arrays\n",
        "            x = Tensor(np.array([1,2,3]))\n",
        "\n",
        "            # a partir de outros tensores (construtor de copia)\n",
        "            x = Tensor(Tensor(np.array([1,2,3])))\n",
        "\n",
        "        Para isto, as seguintes regras devem ser obedecidas:\n",
        "\n",
        "        - Se o argumento arr n√£o for um array do numpy,\n",
        "            ele deve ser convertido em um. Defina o dtype do\n",
        "            array como float de forma a permitir que N√ÉO seja\n",
        "            necess√°rio passar constantes float como Tensor(3.0),\n",
        "            mas possamos criar um tensor apenas com Tensor(3).\n",
        "\n",
        "        - O atributo _arr deve ser uma matriz, isto √©,\n",
        "            ter 2 dimens√µes (ver Regras).\n",
        "\n",
        "        - Se o argumento arr for um Tensor, ele deve ser\n",
        "            copiado (cuidado com c√≥pias por refer√™ncia).\n",
        "\n",
        "        - Se arr for um array do numpy com 1 dimens√£o,\n",
        "            ele deve ser convertido em uma matriz coluna.\n",
        "\n",
        "        - Se arr for um array do numpy com dimens√£o maior\n",
        "            que 2, deve ser lan√ßada uma exce√ß√£o.\n",
        "\n",
        "        - Tensores que n√£o foram produzidos como resultado\n",
        "            de uma opera√ß√£o n√£o t√™m pais nem opera√ß√£o.\n",
        "            Os nomes destes tensores devem seguir o formato in:3.\n",
        "        \"\"\"\n",
        "\n",
        "       # Se arr for Tensor ‚Üí c√≥pia (construtor de c√≥pia)\n",
        "        if isinstance(arr, Tensor):\n",
        "            self._arr = arr._arr.copy()\n",
        "\n",
        "        else:\n",
        "            # Converte para numpy array float\n",
        "            if not isinstance(arr, np.ndarray):\n",
        "                arr = np.array(arr, dtype=float)\n",
        "            else:\n",
        "                arr = arr.astype(float)\n",
        "\n",
        "            # Escalar ‚Üí matriz 1x1\n",
        "            if arr.ndim == 0:\n",
        "                arr = arr.reshape(1, 1)\n",
        "\n",
        "            # Vetor ‚Üí matriz coluna\n",
        "            elif arr.ndim == 1:\n",
        "                arr = arr.reshape(arr.shape[0], 1)\n",
        "\n",
        "            # Mais de 2 dimens√µes ‚Üí erro\n",
        "            elif arr.ndim > 2:\n",
        "                raise ValueError(\"Tensor deve ser sempre uma matriz (2D)\")\n",
        "\n",
        "            self._arr = arr\n",
        "\n",
        "\n",
        "        # if operation is None:\n",
        "        #   self._name = NameManager.new('in')\n",
        "        # else:\n",
        "        #   self._name = NameManager.new(operation.name)\n",
        "\n",
        "        #metadados do tensor\n",
        "        self._parents = parents\n",
        "        self.requires_grad = requires_grad\n",
        "        self.operation = operation\n",
        "        self.grad = None\n",
        "\n",
        "        # # -------- NOME --------\n",
        "        if name != '':\n",
        "            # print(\"name:\"+name)\n",
        "            self._name = name\n",
        "        else:\n",
        "            if not parents and operation is None:\n",
        "                # print(\"name_in:\")\n",
        "                self._name = NameManager.new('in')\n",
        "            else:\n",
        "                self._name = NameManager.new(operation.name)\n",
        "\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"Reinicia o gradiente com zero\"\"\"\n",
        "        self.grad = Tensor(np.zeros_like(self._arr))\n",
        "\n",
        "    def numpy(self):\n",
        "        \"\"\"Retorna o array interno\"\"\"\n",
        "        return self._arr\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Permite visualizar os dados do tensor como string\"\"\"\n",
        "        return f\"Tensor({self._arr}, name={self._name}, shape={self._arr.shape})\"\n",
        "\n",
        "    def backward(self, my_grad=None):\n",
        "        \"\"\"M√©todo usado tanto iniciar o processo de\n",
        "        diferencia√ß√£o autom√°tica, quanto por um filho\n",
        "        para enviar o gradiente do pai. No primeiro\n",
        "        caso, o argumento my_grad n√£o ser√° passado.\n",
        "        \"\"\"\n",
        "        #caso inicial\n",
        "        if my_grad is None:\n",
        "          my_grad = Tensor(np.ones_like(self._arr))\n",
        "          # my_grad = np.ones_like(self._arr)\n",
        "\n",
        "        # primeira chegada de grad\n",
        "        if self.grad is None:\n",
        "            if self.operation is None:\n",
        "                # √â tensor de entrada ‚Üí in_grad\n",
        "                self.grad = Tensor(my_grad.numpy(),name=NameManager.new(\"in_grad\"),requires_grad=False)\n",
        "            else:\n",
        "                # √â tensor intermedi√°rio ‚Üí grad normal\n",
        "                self.grad = Tensor(my_grad.numpy(),requires_grad=False)\n",
        "        else:\n",
        "            # acumula gradiente\n",
        "            self.grad = Tensor(my_grad.numpy() + self.grad.numpy(),requires_grad=False)\n",
        "\n",
        "\n",
        "        # Verificando se √© tensor de entrada\n",
        "        if self.operation is None:\n",
        "          return\n",
        "\n",
        "        # Calcula gradientes locais via opera√ß√£o\n",
        "        grads = self.operation.grad(self.grad, *self._parents)\n",
        "\n",
        "        # Propaga para os pais\n",
        "        for parent, g in zip(self._parents, grads):\n",
        "            if parent.requires_grad:\n",
        "                parent.backward(g)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2c612fc",
      "metadata": {
        "id": "d2c612fc"
      },
      "source": [
        "### Interface de  Opera√ß√µes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7db44044",
      "metadata": {
        "id": "7db44044"
      },
      "source": [
        "A classe abaixo define a interface que as opera√ß√µes devem implementar. Ela n√£o precisa ser modificada, mas pode, caso queira."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28a19b73",
      "metadata": {
        "tags": [
          "op"
        ],
        "id": "28a19b73"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Op(ABC):\n",
        "    name: str = ''\n",
        "    @abstractmethod\n",
        "    def __call__(self, *args, **kwargs) -> Tensor:\n",
        "        \"\"\"Realiza a opera√ß√£o usando as entradas e\n",
        "            retorna o tensor resultado. O m√©todo deve\n",
        "            garantir que o atributo parents do tensor\n",
        "            de sa√≠da seja uma lista de tensores.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
        "        \"\"\"Retorna os gradientes dos pais em como tensores.\n",
        "\n",
        "        Arguments:\n",
        "\n",
        "        - back_grad: Derivada parcial em rela√ß√£o √† sa√≠da\n",
        "            da opera√ß√£o backpropagada pelo filho.\n",
        "\n",
        "        - args: variaveis de entrada da operacao (pais)\n",
        "            como tensores.\n",
        "\n",
        "        - O nome dos tensores de gradiente devem ter o\n",
        "            nome da operacao seguido de '_grad'.\n",
        "        \"\"\"\n",
        "\n",
        "    def _ts(self, *args) -> list[Tensor]:\n",
        "        vals = []\n",
        "        for a in args:\n",
        "            if isinstance(a, Tensor):\n",
        "                vals.append(a)\n",
        "            else:\n",
        "                vals.append(Tensor(a, requires_grad=False))\n",
        "        return vals\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b89e386",
      "metadata": {
        "id": "9b89e386"
      },
      "source": [
        "### Implementa√ß√£o das Opera√ß√µes\n",
        "\n",
        "Opera√ß√µes devem herdar de `Op` e implementar os m√©todos `__call__` e `grad`.\n",
        "\n",
        "Pelo menos as seguintes opera√ß√µes devem ser implementadas:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa4f7719",
      "metadata": {
        "tags": [
          "add"
        ],
        "id": "aa4f7719"
      },
      "outputs": [],
      "source": [
        "class Add(Op):\n",
        "    \"\"\"Add(a, b): a + b\"\"\"\n",
        "    name = \"add\"\n",
        "    def __call__(self, *args, **kwargs) -> Tensor:\n",
        "        \"\"\"Realiza a opera√ß√£o usando os argumentos dados em args\"\"\"\n",
        "        args = self._ts(*args)\n",
        "        result = args[0].numpy() + args[1].numpy()\n",
        "        return Tensor(result, parents=args, name=NameManager.new('add'), operation=self)\n",
        "\n",
        "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
        "        return [Tensor(back_grad.numpy(), name=NameManager.new('add_grad')),\n",
        "                Tensor(back_grad.numpy(), name=NameManager.new('add_grad'))]\n",
        "\n",
        "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
        "add = Add()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05cb44e6",
      "metadata": {
        "tags": [
          "sub"
        ],
        "id": "05cb44e6"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Sub(Op):\n",
        "    \"\"\"Sub(a, b): a - b\"\"\"\n",
        "    # name = \"sub\"\n",
        "    def __call__(self, *args, **kwargs) -> Tensor:\n",
        "        \"\"\"Realiza a opera√ß√£o usando os argumentos dados em args\"\"\"\n",
        "        args = self._ts(*args)\n",
        "        result = args[0].numpy() - args[1].numpy()\n",
        "        return Tensor(result, parents=args, name=NameManager.new('sub'), operation=self)\n",
        "\n",
        "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
        "        \"\"\"Retorna a lista de derivadas parciais em rela√ß√£o aos pais (passados em args)\"\"\"\n",
        "        arg = self._ts(*args)\n",
        "        return [Tensor(back_grad.numpy(), name=NameManager.new('sub_grad')),\n",
        "                Tensor(-back_grad.numpy(), name=NameManager.new('sub_grad'))]\n",
        "\n",
        "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
        "sub = Sub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f53df08",
      "metadata": {
        "tags": [
          "prod"
        ],
        "id": "6f53df08"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Prod(Op):\n",
        "    \"\"\"Prod(a, b): produto ponto a ponto de a e b ou produto escalar-tensor\"\"\"\n",
        "    def __call__(self, *args, **kwargs) -> Tensor:\n",
        "        args = self._ts(*args)\n",
        "        result = args[0].numpy() * args[1].numpy()\n",
        "        return Tensor(result, parents=args, name=NameManager.new('prod'), operation=self)\n",
        "\n",
        "\n",
        "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
        "        \"\"\"Retorna a lista de derivadas parciais em rela√ß√£o aos pais (passados em args)\"\"\"\n",
        "        arg = self._ts(*args)\n",
        "        return [Tensor(arg[1].numpy()*back_grad.numpy(), name=NameManager.new('prod_grad')),\n",
        "                Tensor(arg[0].numpy()*back_grad.numpy(), name=NameManager.new('prod_grad'))]\n",
        "\n",
        "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
        "prod = Prod()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f3838a7",
      "metadata": {
        "tags": [
          "sin"
        ],
        "id": "8f3838a7"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Sin(Op):\n",
        "    \"\"\"seno element-wise\"\"\"\n",
        "    def __call__(self, *args, **kwargs) -> Tensor:\n",
        "        args = self._ts(*args)\n",
        "        result = np.sin(args[0].numpy())\n",
        "        return Tensor(result, parents=args, name=NameManager.new('sin'), operation=self)\n",
        "\n",
        "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
        "        arg = self._ts(*args)\n",
        "        return [Tensor(np.cos(arg[0].numpy())*back_grad.numpy(), name=NameManager.new('sin_grad'))]\n",
        "\n",
        "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
        "sin = Sin()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "138cb8ef",
      "metadata": {
        "tags": [
          "cos"
        ],
        "id": "138cb8ef"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Cos(Op):\n",
        "    \"\"\"cosseno element-wise\"\"\"\n",
        "    def __call__(self, *args, **kwargs) -> Tensor:\n",
        "        args = self._ts(*args)\n",
        "        result = np.cos(args[0].numpy())\n",
        "        return Tensor(result, parents=args, name=NameManager.new('cos'), operation=self)\n",
        "\n",
        "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
        "        args = self._ts(*args)\n",
        "        return [Tensor((-np.sin(args[0].numpy()))*back_grad.numpy(), name=NameManager.new('cos_grad'))]\n",
        "\n",
        "\n",
        "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
        "cos = Cos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46eac52c",
      "metadata": {
        "tags": [
          "sum"
        ],
        "id": "46eac52c"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Sum(Op):\n",
        "    \"\"\"Retorna a soma dos elementos do tensor\"\"\"\n",
        "    def __call__(self, *args, **kwargs) -> Tensor:\n",
        "        args = self._ts(*args)\n",
        "        result = np.sum(args[0].numpy())\n",
        "        return Tensor(result, parents=args, name=NameManager.new('sum'), operation=self)\n",
        "\n",
        "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
        "        arg = self._ts(*args)\n",
        "        return [Tensor(np.ones_like(arg[0].numpy())*back_grad.numpy(), name=NameManager.new('sum_grad'))]\n",
        "\n",
        "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
        "# ‚ö†Ô∏è vamos chamar de my_sum porque python ja possui uma funcao sum\n",
        "my_sum = Sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e098a39c",
      "metadata": {
        "tags": [
          "mean"
        ],
        "id": "e098a39c"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Mean(Op):\n",
        "    \"\"\"Retorna a m√©dia dos elementos do tensor\"\"\"\n",
        "    def __call__(self, *args, **kwargs) -> Tensor:\n",
        "        args = self._ts(*args)\n",
        "        result = np.average(args[0].numpy())\n",
        "        return Tensor(result, parents=args, name=NameManager.new('mean'), operation=self)\n",
        "\n",
        "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
        "        arg = self._ts(*args)\n",
        "\n",
        "        x = arg[0].numpy()\n",
        "        n = x.size  # n√∫mero de elementos do tensor original\n",
        "        grad_value = np.ones_like(x) * (back_grad.numpy() / n)\n",
        "\n",
        "        return [Tensor((np.ones_like(arg[0].numpy())*back_grad.numpy())/arg[0].numpy().size, name=NameManager.new('avr_grad'))]\n",
        "\n",
        "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
        "mean = Mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37692879",
      "metadata": {
        "tags": [
          "square"
        ],
        "id": "37692879"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Square(Op):\n",
        "    \"\"\"Eleva cada elemento ao quadrado\"\"\"\n",
        "    def __call__(self, *args, **kwargs) -> Tensor:\n",
        "        args = self._ts(*args)\n",
        "        result = np.square(args[0].numpy())\n",
        "        return Tensor(result, parents=args, name=NameManager.new('sqr'), operation=self)\n",
        "\n",
        "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
        "        arg = self._ts(*args)\n",
        "        return [Tensor(2*arg[0].numpy()*back_grad.numpy(), name=NameManager.new('sqr_grad'))]\n",
        "\n",
        "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
        "square = Square()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6542807d",
      "metadata": {
        "tags": [
          "matmul"
        ],
        "id": "6542807d"
      },
      "outputs": [],
      "source": [
        "# from numpy._core.defchararray import translate\n",
        "\n",
        "class MatMul(Op):\n",
        "    \"\"\"MatMul(A, B): multiplica√ß√£o de matrizes\n",
        "\n",
        "    C = A @ B\n",
        "    de/dA = de/dc @ B^T\n",
        "    de/dB = A^T @ de/dc\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, *args, **kwargs) -> Tensor:\n",
        "        args1, args2 = self._ts(*args)\n",
        "        result = args1.numpy() @ args2.numpy()\n",
        "        return Tensor(result, parents=[args1,args2], name=NameManager.new('matMul'), operation=self)\n",
        "\n",
        "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
        "        a, b = self._ts(*args)\n",
        "\n",
        "        da = back_grad.numpy() @ b.numpy().T\n",
        "        db = a.numpy().T @ back_grad.numpy()\n",
        "\n",
        "        return [Tensor(da, name=NameManager.new('matMul_grad')),\n",
        "                Tensor(db, name=NameManager.new('matMul_grad'))]\n",
        "\n",
        "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
        "matmul = MatMul()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c08a38e3",
      "metadata": {
        "tags": [
          "exp"
        ],
        "id": "c08a38e3"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Exp(Op):\n",
        "    \"\"\"Exponencia√ß√£o element-wise\"\"\"\n",
        "    def __call__(self, *args, **kwargs) -> Tensor:\n",
        "        args = self._ts(*args)\n",
        "\n",
        "        result = np.exp(args[0].numpy())\n",
        "        return Tensor(result, parents=args, name=NameManager.new('exp'), operation=self)\n",
        "\n",
        "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
        "        args = self._ts(*args)\n",
        "        return [Tensor(np.exp(args[0].numpy())*back_grad.numpy(), name=NameManager.new('sqr_grad'))]\n",
        "\n",
        "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
        "exp = Exp()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1acc813e",
      "metadata": {
        "tags": [
          "relu"
        ],
        "id": "1acc813e"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ReLU(Op):\n",
        "    \"\"\"ReLU element-wise\"\"\"\n",
        "    def __call__(self, *args, **kwargs) -> Tensor:\n",
        "        args = self._ts(*args)\n",
        "        result = np.maximum(0, args[0].numpy())\n",
        "\n",
        "        return Tensor(result, parents=args, name=NameManager.new('relu'), operation=self)\n",
        "\n",
        "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
        "        args = self._ts(*args)\n",
        "\n",
        "        # Derivada da ReLU: 1 onde x>0, sen√£o 0\n",
        "        relu_derivative = (args[0].numpy() > 0).astype(float)\n",
        "\n",
        "        return [Tensor(relu_derivative*back_grad.numpy(), name=NameManager.new('relu_grad'))]\n",
        "\n",
        "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
        "relu = ReLU()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae499275",
      "metadata": {
        "tags": [
          "sigmoid"
        ],
        "id": "ae499275"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Sigmoid(Op):\n",
        "    \"\"\"Sigmoid element-wise\"\"\"\n",
        "    def __call__(self, *args, **kwargs) -> Tensor:\n",
        "        args = self._ts(*args)\n",
        "        result = 1.0 / ( 1.0 + np.exp(-args[0].numpy()))\n",
        "\n",
        "        return Tensor(result, parents=args, name=NameManager.new('sigmoid'), operation=self)\n",
        "\n",
        "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
        "        args = self._ts(*args)\n",
        "\n",
        "        # ùë†ùëñùëîùëö(ùë•) ‚àó (1 ‚àí ùë†ùëñùëîùëö(ùë•))\n",
        "        sig_derivative = (1.0 / ( 1.0 + np.exp(-args[0].numpy()))) * (1.0 - (1.0 / ( 1.0 + np.exp(-args[0].numpy()))))\n",
        "\n",
        "        return [Tensor(sig_derivative*back_grad.numpy(), name=NameManager.new('sig_grad'))]\n",
        "\n",
        "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
        "sigmoid = Sigmoid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ce464ba",
      "metadata": {
        "tags": [
          "tanh"
        ],
        "id": "6ce464ba"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Tanh(Op):\n",
        "    \"\"\"Tanh element-wise\"\"\"\n",
        "    def __call__(self, *args, **kwargs) -> Tensor:\n",
        "        args = self._ts(*args)\n",
        "\n",
        "        # tanh z = e^z ‚àí e^-z / e^z + e^-z\n",
        "        result = (np.exp(args[0].numpy()) - np.exp(-args[0].numpy())) / (np.exp(args[0].numpy()) + np.exp(-args[0].numpy()))\n",
        "\n",
        "        return Tensor(result, parents=args, name=NameManager.new('tanh'), operation=self)\n",
        "\n",
        "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
        "        args = self._ts(*args)\n",
        "\n",
        "        # 1 - tanh(z)¬≤\n",
        "        tanh_derivative = 1.0 - (np.exp(args[0].numpy()) - np.exp(-args[0].numpy())) / (np.exp(args[0].numpy()) + np.exp(-args[0].numpy()))\\\n",
        "                                *(np.exp(args[0].numpy()) - np.exp(-args[0].numpy())) / (np.exp(args[0].numpy()) + np.exp(-args[0].numpy()))\n",
        "\n",
        "\n",
        "        return [Tensor(tanh_derivative*back_grad.numpy(), name=NameManager.new('tanh_grad'))]\n",
        "\n",
        "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
        "tanh = Tanh()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f15a37eb",
      "metadata": {
        "tags": [
          "softmax"
        ],
        "id": "f15a37eb"
      },
      "outputs": [],
      "source": [
        "class Softmax(Op):\n",
        "    \"\"\"Softmax de um array de valores. Lembre-se que cada elemento do array influencia o resultado da fun√ß√£o para todos os demais elementos.\"\"\"\n",
        "    def __call__(self, *args, **kwargs) -> Tensor:\n",
        "        args = self._ts(*args)\n",
        "\n",
        "        # Evita overflow\n",
        "        shifted = args[0].numpy() - np.max(args[0].numpy())\n",
        "        exp_vals = np.exp(shifted)\n",
        "        result = exp_vals / np.sum(exp_vals)\n",
        "\n",
        "        return Tensor(result, parents=args, name=NameManager.new('softmax'), operation=self)\n",
        "\n",
        "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
        "        x = self._ts(*args)[0]\n",
        "\n",
        "        # Recalcula softmax\n",
        "        shifted = x.numpy() - np.max(x.numpy())\n",
        "        exp_vals = np.exp(shifted)\n",
        "        s = exp_vals / np.sum(exp_vals)  # vetor softmax\n",
        "\n",
        "        # Produto interno s_j * back_grad_j\n",
        "        dot = np.sum(s * back_grad.numpy())\n",
        "\n",
        "        # F√≥rmula: grad_i = s_i * (back_grad_i - dot)\n",
        "        grad_x = s * (back_grad.numpy() - dot)\n",
        "\n",
        "        return [Tensor(grad_x,name=NameManager.new(\"softmax_grad\"))]\n",
        "\n",
        "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
        "softmax = Softmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12256fd7",
      "metadata": {
        "id": "12256fd7"
      },
      "source": [
        "\n",
        "### ‚ÄºÔ∏è Regras e Pontos de Aten√ß√£o‚ÄºÔ∏è\n",
        "\n",
        "- Vamos fazer a hip√≥tese simplificadora que Tensores devem ser sempre matrizes. Por exemplo, o escalar 2 deve ser armazado em `_arr` como a matriz `[[2]]`. De forma similar, a lista `[1, 2, 3]` deve ser armazenada em `_arr` como em uma matriz coluna.\n",
        "\n",
        "- Devem ser realizados `asserts` nas opera√ß√µes para garantir que os shapes dos operandos fazem sentido. Esta verifica√ß√£o tamb√©m deve ser feita depois das opera√ß√µes que manipulam gradientes de tensores.\n",
        "\n",
        "- Devem ser respeitados os nomes dos atributos, m√©todos e classes para viabilizar os testes autom√°ticos.\n",
        "\n",
        "- Gradientes devem ser calculados usando uma passada pelo grafo computacional.\n",
        "\n",
        "- Os gradientes devem ser somados e n√£o substitu√≠dos nas chamadas de  backward. Isto vai permitir que os gradientes sejam acumulados entre amostras do dataset e que os resultados sejam corretos mesmo em caso de ramifica√ß√µes e jun√ß√µes no grafo computacional.\n",
        "\n",
        "- Lembre-se de zerar os gradientes ap√≥s cada passo de gradient descent (atualiza√ß√£o dos par√¢metros).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc927248",
      "metadata": {
        "id": "fc927248"
      },
      "source": [
        "## Testes B√°sicos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5ae08a8",
      "metadata": {
        "id": "c5ae08a8"
      },
      "source": [
        "Estes testes avaliam se a derivada da fun√ß√£o est√° sendo calculada corretamente, mas em muitos casos **n√£o** avaliam se os gradientes backpropagados est√£o sendo incorporados corretamente. Esta avalia√ß√£o ser√° feita nos problemas da pr√≥xima se√ß√£o."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b05318a9",
      "metadata": {
        "id": "b05318a9"
      },
      "source": [
        "Operador de Soma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd20550",
      "metadata": {
        "tags": [
          "test_add"
        ],
        "id": "9fd20550",
        "outputId": "b0e2e68b-8e84-4ca8-9ff4-30ca2ba05017",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor([[1.]\n",
            " [1.]\n",
            " [1.]], name=in_grad:0, shape=(3, 1))\n",
            "Tensor([[1.]\n",
            " [1.]\n",
            " [1.]], name=in_grad:1, shape=(3, 1))\n"
          ]
        }
      ],
      "source": [
        "# add\n",
        "\n",
        "a = Tensor([1.0, 2.0, 3.0])\n",
        "b = Tensor([4.0, 5.0, 6.0])\n",
        "c = add(a, b)\n",
        "d = add(c, 3.0)\n",
        "d.backward()\n",
        "\n",
        "# esperado: matrizes coluna contendo 1\n",
        "print(a.grad)\n",
        "print(b.grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac72b1a",
      "metadata": {
        "id": "fac72b1a"
      },
      "source": [
        "Operador de Subtra√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612377aa",
      "metadata": {
        "tags": [
          "test_sub"
        ],
        "id": "612377aa",
        "outputId": "b255a7f7-7a00-41eb-c071-13cb9a43ecad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor([[1.]\n",
            " [1.]\n",
            " [1.]], name=in_grad:2, shape=(3, 1))\n",
            "Tensor([[-1.]\n",
            " [-1.]\n",
            " [-1.]], name=in_grad:3, shape=(3, 1))\n"
          ]
        }
      ],
      "source": [
        "# sub\n",
        "\n",
        "a = Tensor([1.0, 2.0, 3.0])\n",
        "b = Tensor([4.0, 5.0, 6.0])\n",
        "c = sub(a, b)\n",
        "d = sub(c, 3.0)\n",
        "d.backward()\n",
        "\n",
        "# esperado: matrizes coluna contendo 1 e -1\n",
        "print(a.grad)\n",
        "print(b.grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c7c8e63",
      "metadata": {
        "id": "9c7c8e63"
      },
      "source": [
        "Operador de Produto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc60de82",
      "metadata": {
        "tags": [
          "test_prod"
        ],
        "id": "dc60de82",
        "outputId": "229d7e5d-07e8-4dd9-ddfe-9b72915495f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor([[12.]\n",
            " [15.]\n",
            " [18.]], name=in_grad:4, shape=(3, 1))\n",
            "Tensor([[3.]\n",
            " [6.]\n",
            " [9.]], name=in_grad:5, shape=(3, 1))\n"
          ]
        }
      ],
      "source": [
        "# prod\n",
        "\n",
        "a = Tensor([1.0, 2.0, 3.0])\n",
        "b = Tensor([4.0, 5.0, 6.0])\n",
        "c = prod(a, b)\n",
        "d = prod(c, 3.0)\n",
        "d.backward()\n",
        "\n",
        "# esperado: [12, 15, 18]^T\n",
        "print(a.grad)\n",
        "# esperado: [3, 6, 9]^T\n",
        "print(b.grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d91e1c3",
      "metadata": {
        "id": "5d91e1c3"
      },
      "source": [
        "Operadores trigonom√©tricos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6185a989",
      "metadata": {
        "tags": [
          "test_sin_cos"
        ],
        "id": "6185a989",
        "outputId": "e78401a4-85bd-42ac-eb70-aab9c6da2150",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor([[-1.]\n",
            " [ 1.]\n",
            " [-1.]], name=in:24, shape=(3, 1))\n"
          ]
        }
      ],
      "source": [
        "# sin e cos\n",
        "\n",
        "a = Tensor([np.pi, 0, np.pi/2])\n",
        "b = sin(a)\n",
        "c = cos(a)\n",
        "d = my_sum(add(b, c))\n",
        "d.backward()\n",
        "\n",
        "# esperado: [-1, 1, -1]^T\n",
        "print(a.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f29f232",
      "metadata": {
        "tags": [
          "test_sum"
        ],
        "id": "5f29f232",
        "outputId": "a6132a5a-ca29-4cc7-c506-ec803241c080",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor([[4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]], name=in:31, shape=(4, 1))\n"
          ]
        }
      ],
      "source": [
        "# Sum\n",
        "\n",
        "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
        "b = add(prod(a, 3.0), a)\n",
        "c = my_sum(b)\n",
        "c.backward()\n",
        "\n",
        "# esperado: [4, 4, 4, 4]^T\n",
        "# print(b.numpy())\n",
        "# print(c.numpy())\n",
        "print(a.grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8943e71a",
      "metadata": {
        "tags": [
          "test_mean"
        ],
        "id": "8943e71a",
        "outputId": "8796ecf8-55b5-46f1-f996-a120c756d9cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor([[0.25]\n",
            " [0.25]\n",
            " [0.25]\n",
            " [0.25]], name=in_grad:8, shape=(4, 1))\n"
          ]
        }
      ],
      "source": [
        "# Mean\n",
        "\n",
        "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
        "b = mean(a)\n",
        "b.backward()\n",
        "\n",
        "# esperado: [0.25, 0.25, 0.25, 0.25]^T\n",
        "print(a.grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c7dbd2c",
      "metadata": {
        "tags": [
          "test_square"
        ],
        "id": "1c7dbd2c",
        "outputId": "503a96a6-45bc-4d4b-92b2-56f752580763",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor([[9.]\n",
            " [1.]\n",
            " [0.]\n",
            " [4.]], name=sqr:0, shape=(4, 1))\n",
            "Tensor([[6.]\n",
            " [2.]\n",
            " [0.]\n",
            " [4.]], name=in_grad:9, shape=(4, 1))\n"
          ]
        }
      ],
      "source": [
        "# Square\n",
        "\n",
        "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
        "b = square(a)\n",
        "\n",
        "# esperado: [9, 1, 0, 4]^T\n",
        "print(b)\n",
        "\n",
        "b.backward()\n",
        "\n",
        "# esperado: [6, 2, 0, 4]\n",
        "print(a.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02f2ead7",
      "metadata": {
        "tags": [
          "test_matmul"
        ],
        "id": "02f2ead7",
        "outputId": "efebded8-264c-4b77-9adc-28940d993007",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor([[14.]\n",
            " [32.]\n",
            " [50.]], name=matMul:0, shape=(3, 1))\n",
            "Tensor([[1. 2. 3.]\n",
            " [1. 2. 3.]\n",
            " [1. 2. 3.]], name=in_grad:10, shape=(3, 3))\n",
            "Tensor([[12.]\n",
            " [15.]\n",
            " [18.]], name=in_grad:11, shape=(3, 1))\n"
          ]
        }
      ],
      "source": [
        "# matmul\n",
        "\n",
        "W = Tensor([\n",
        "    [1.0, 2.0, 3.0],\n",
        "    [4.0, 5.0, 6.0],\n",
        "    [7.0, 8.0, 9.0]\n",
        "])\n",
        "\n",
        "v = Tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "z = matmul(W, v)\n",
        "\n",
        "# esperado: [14, 32, 50]^T\n",
        "print(z)\n",
        "\n",
        "z.backward()\n",
        "\n",
        "# esperado:\n",
        "# [1, 2, 3]\n",
        "# [1, 2, 3]\n",
        "# [1, 2, 3]\n",
        "print(W.grad)\n",
        "\n",
        "# esperado: [12, 15, 18]^T\n",
        "print(v.grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "706212d2",
      "metadata": {
        "tags": [
          "test_exp"
        ],
        "id": "706212d2",
        "outputId": "569f43b2-c0fa-48a2-ff6b-abca83c61a28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor([[ 2.71828183]\n",
            " [ 7.3890561 ]\n",
            " [20.08553692]], name=exp:0, shape=(3, 1))\n",
            "Tensor([[ 2.71828183]\n",
            " [ 7.3890561 ]\n",
            " [20.08553692]], name=in_grad:12, shape=(3, 1))\n"
          ]
        }
      ],
      "source": [
        "# Exp\n",
        "\n",
        "v = Tensor([1.0, 2.0, 3.0])\n",
        "w = exp(v)\n",
        "\n",
        "# esperado: [2.718..., 7.389..., 20.085...]^T\n",
        "print(w)\n",
        "\n",
        "w.backward()\n",
        "\n",
        "# esperado: [2.718..., 7.389..., 20.085...]^T\n",
        "print(v.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9510d010",
      "metadata": {
        "tags": [
          "test_relu"
        ],
        "id": "9510d010",
        "outputId": "00d060b4-7dc1-4e1b-a63f-b8909b7e8f3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor([[0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [3.]], name=relu:0, shape=(4, 1))\n",
            "Tensor([[0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]], name=in_grad:13, shape=(4, 1))\n"
          ]
        }
      ],
      "source": [
        "# Relu\n",
        "\n",
        "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
        "w = relu(v)\n",
        "\n",
        "# esperado: [0, 0, 1, 3]^T\n",
        "print(w)\n",
        "\n",
        "w.backward()\n",
        "\n",
        "# esperado: [0, 0, 1, 1]^T\n",
        "print(v.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f0fbf8d",
      "metadata": {
        "tags": [
          "test_sigmoid"
        ],
        "id": "2f0fbf8d",
        "outputId": "babfd112-1230-4121-f12f-ecdb87c1c4a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor([[0.26894142]\n",
            " [0.5       ]\n",
            " [0.73105858]\n",
            " [0.95257413]], name=sigmoid:0, shape=(4, 1))\n",
            "Tensor([[0.19661193]\n",
            " [0.25      ]\n",
            " [0.19661193]\n",
            " [0.04517666]], name=in_grad:14, shape=(4, 1))\n"
          ]
        }
      ],
      "source": [
        "# Sigmoid\n",
        "\n",
        "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
        "w = sigmoid(v)\n",
        "\n",
        "# esperado: [0.268.., 0.5, 0.731.., 0.952..]^T\n",
        "print(w)\n",
        "\n",
        "w.backward()\n",
        "\n",
        "# esperado: [0.196..., 0.25, 0.196..., 0.045...]^T\n",
        "print(v.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e867dec",
      "metadata": {
        "tags": [
          "test_tanh"
        ],
        "id": "7e867dec",
        "outputId": "932c5303-b2b8-4c86-83fa-146502da9ff0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor([[-0.76159416]\n",
            " [ 0.        ]\n",
            " [ 0.76159416]\n",
            " [ 0.99505475]], name=tanh:0, shape=(4, 1))\n",
            "Tensor([[0.41997434]\n",
            " [1.        ]\n",
            " [0.41997434]\n",
            " [0.00986604]], name=in_grad:15, shape=(4, 1))\n"
          ]
        }
      ],
      "source": [
        "# Tanh\n",
        "\n",
        "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
        "w = tanh(v)\n",
        "\n",
        "# esperado: [[-0.76159416, 0., 0.76159416, 0.99505475]^T\n",
        "print(w)\n",
        "\n",
        "w.backward()\n",
        "\n",
        "# esperado: [0.41997434, 1., 0.41997434, 0.00986604]^T\n",
        "print(v.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fd3235d",
      "metadata": {
        "tags": [
          "test_softmax"
        ],
        "id": "7fd3235d",
        "outputId": "37788b46-683c-4f9a-fe51-fd73fb222a8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor([[0.00381737]\n",
            " [0.13970902]\n",
            " [0.23034123]\n",
            " [0.62613238]], name=softmax:0, shape=(4, 1))\n",
            "MSE: Tensor([[0.36424932]], name=mean:1, shape=(1, 1))\n",
            "Tensor([[-0.00278095]\n",
            " [-0.02243068]\n",
            " [-0.02654377]\n",
            " [ 0.05175539]], name=in_grad:16, shape=(4, 1))\n"
          ]
        }
      ],
      "source": [
        "# Softmax\n",
        "\n",
        "x = Tensor([-3.1, 0.5, 1.0, 2.0])\n",
        "y = softmax(x)\n",
        "\n",
        "# esperado: [0.00381737, 0.13970902, 0.23034123, 0.62613238]^T\n",
        "print(y)\n",
        "\n",
        "# como exemplo, calcula o MSE para um target vector\n",
        "diff = sub(y, [1, 0, 0, 0])\n",
        "sq = square(diff)\n",
        "a = mean(sq)\n",
        "\n",
        "# esperado: 0.36424932\n",
        "print(\"MSE:\", a)\n",
        "\n",
        "a.backward()\n",
        "\n",
        "# esperado: [-0.00278095, -0.02243068, -0.02654377, 0.05175539]^T\n",
        "print(x.grad)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c27a3d9f",
      "metadata": {
        "id": "c27a3d9f"
      },
      "source": [
        "\n",
        "## Refer√™ncias\n",
        "\n",
        "### Principais\n",
        "\n",
        "- [Build your own pytorch](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-1-Computation-Graphs/)\n",
        "- [Build your own Pytorch - 2: Backpropagation](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-2-Autograd/)\n",
        "- [Build your own PyTorch - 3: Training a Neural Network with self-made AD software](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-3-Build-Classifier/)\n",
        "- [Pytorch: A Gentle Introduction to torch.autograd](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
        "- [Automatic Differentiation with torch.autograd](https://docs.pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)\n",
        "\n",
        "### Secund√°rias\n",
        "\n",
        "- [Tom Roth: Building a computational graph: part 1](https://tomroth.dev/compgraph1/)\n",
        "- [Tom Roth: Building a computational graph: part 2](https://tomroth.dev/compgraph2/)\n",
        "- [Tom Roth: Building a computational graph: part 3](https://tomroth.dev/compgraph3/)\n",
        "- [Roger Grosse (Toronto) class on Automatic Differentiation](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf)\n",
        "- [Computational graphs and gradient flows](https://simple-english-machine-learning.readthedocs.io/en/latest/neural-networks/computational-graphs.html)\n",
        "- [Colah Visual Blog: Backprop](https://colah.github.io/posts/2015-08-Backprop/)\n",
        "- [Towards Data Science: Automatic Differentiation (AutoDiff): A Brief Intro with Examples](https://towardsdatascience.com/automatic-differentiation-autodiff-a-brief-intro-with-examples-3f3d257ffe3b/)\n",
        "- [A Hands-on Introduction to Automatic Differentiation - Part 1](https://mostafa-samir.github.io/auto-diff-pt1/)\n",
        "- [Build Your own Deep Learning Framework - A Hands-on Introduction to Automatic Differentiation - Part 2](https://mostafa-samir.github.io/auto-diff-pt1/)\n"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "venv_main",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}